services:
  # Main BEAM application container
  beam-core:
    build:
      context: .
      dockerfile: docker/Dockerfile.beam-core
    container_name: beam-core
    volumes:
      - ./data:/app/data  # Mount host data directory as read-write
      - ./models:/app/models  # Mount host models directory as read-write
      - ./predictions:/app/predictions  # Mount host predictions directory as read-write
      - ./logs:/app/logs  # Mount logs directory to host
    environment:
      - GEMINI_API_KEY=${GEMINI_API_KEY:-}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - PYTHONPATH=/app/src
      - BEAM_LOG_PATH=/app/logs/beam.log
    depends_on:
      - zeek-processor
      - database
    networks:
      - beam-network
    stdin_open: true
    tty: true

  # Zeek network analysis container
  zeek-processor:
    build:
      context: .
      dockerfile: docker/Dockerfile.zeek
    container_name: zeek-processor
    volumes:
      - ./data:/app/data
    networks:
      - beam-network
    command: ["tail", "-f", "/dev/null"]  # Keep container running

  # Database services container
  database:
    build:
      context: .
      dockerfile: docker/Dockerfile.database
    container_name: beam-database
    volumes:
      - ./data:/app/data
      - db-data:/var/lib/beam
    networks:
      - beam-network
    environment:
      - PYTHONPATH=/app/src

  # Llama model service container
  llama-model:
    image: ollama/ollama:latest
    container_name: beam-llama
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - beam-network
    restart: unless-stopped

  # Optional: Single container for simpler deployments
  beam-all-in-one:
    build:
      context: .
      dockerfile: docker/Dockerfile.all-in-one
    container_name: beam-all-in-one
    volumes:
      - ./data:/app/data  # Mount host data directory as read-write
      - ./models:/app/models  # Mount host models directory as read-write
      - ./predictions:/app/predictions  # Mount host predictions directory as read-write
      - ./logs:/app/logs  # Mount logs directory to host
    environment:
      - GEMINI_API_KEY=${GEMINI_API_KEY:-}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - PYTHONPATH=/app/src
      - BEAM_LOG_PATH=/app/logs/beam.log
    networks:
      - beam-network
    stdin_open: true
    tty: true
    profiles:
      - single-container

volumes:
  db-data:
  ollama-data:

networks:
  beam-network:
    driver: bridge
